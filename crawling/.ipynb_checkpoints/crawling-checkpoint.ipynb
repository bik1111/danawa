{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8f226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# # 차례\n",
    "# ### 1. 크롤링을 통한 상품 정보 및 리뷰 수집\n",
    "# ### 2. CSV 변환\n",
    "# ### 3. 데이터 전처리 (한글만 filtering, 띄어쓰기 수정,\n",
    "# ### 3. Embedding Vector\n",
    "# ### 4. Cosine Similarity 구하기\n",
    "# ### 5. chatGPT 연동\n",
    "\n",
    "# # 1. 크롤링을 통한 상품 정보 및 리뷰 수집\n",
    "\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "data_list = []\n",
    "\n",
    "def crawl_page(url):\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for page_number in range(1, 7):  # Crawl up to 10 pages\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        products = soup.select('div.product_item__MDtDF')\n",
    "\n",
    "        for v in products:\n",
    "            item_name = v.select_one('a.product_link__TrAac.linkAnchor').get('title')\n",
    "            item_price = v.select_one('span.price_num__S2p_v em').text\n",
    "            item_specs = v.select('a.product_detail__oWDMs.product_bar__dHjkA.linkAnchor')\n",
    "            specs = ', '.join([item_spec.text for item_spec in item_specs])\n",
    "            item_href = v.select_one('a.product_link__TrAac.linkAnchor').get('href')\n",
    "\n",
    "            data_list.append({\n",
    "                \"product_name\": item_name,\n",
    "                \"product_price\": item_price,\n",
    "                \"product_spec\": specs,\n",
    "                \"product_link_url\": item_href,\n",
    "            })\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        # Click the next page button\n",
    "        next_page_selector = f\"#content > div.style_content__xWg5l > div.pagination_pagination__fsf34 > div > a:nth-child({page_number + 1})\"\n",
    "        next_page_button = driver.find_element(By.CSS_SELECTOR, next_page_selector)\n",
    "        next_page_button.click()\n",
    "        time.sleep(5)  # Add a delay to allow the next page to load\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "def review_crawl():\n",
    "    for item in data_list:\n",
    "        link_url = item['product_link_url']\n",
    "\n",
    "        # 새로운 HTTP 요청을 보내서 리뷰 페이지를 크롤링\n",
    "        response = requests.get(link_url)\n",
    "        review_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # 리뷰 아이템 선택\n",
    "        reviews = review_soup.select('p.reviewItems_text__XrSSf')\n",
    "        time.sleep(1)\n",
    "        # 정규 표현식을 사용하여 한글만 남기기\n",
    "        cleaned_reviews = [re.sub(r'[^가-힣\\s]', '', review.get_text()) for review in reviews]\n",
    "\n",
    "        # 리뷰를 해당 상품의 딕셔너리에 추가\n",
    "        item['reviews'] = cleaned_reviews\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "# Initial URL\n",
    "initial_url = \"https://search.shopping.naver.com/search/category/100005307\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_page(initial_url)\n",
    "    time.sleep(3)\n",
    "    review_crawl()\n",
    "\n",
    "\n",
    "\n",
    "# # 2. CSV 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb149e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def save_to_csv(data_list, file_path='output.csv'):\n",
    "    with open(file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data_list[0].keys())\n",
    "\n",
    "        # CSV 파일 헤더 쓰기\n",
    "        writer.writeheader()\n",
    "\n",
    "        # 데이터 쓰기\n",
    "        writer.writerows(data_list)\n",
    "\n",
    "save_to_csv(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 DataFrame으로 읽기\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# DataFrame 확인\n",
    "print(df)\n",
    "\n",
    "\n",
    "# # 3. 데이터 전처리\n",
    "\n",
    "# ### 3-1) 한글만 남기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_word(text):\n",
    "    hangul = re.compile('[^가-힣]')\n",
    "    result = hangul.sub(' ', text)\n",
    "    return result\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(lambda x: extract_word(x))\n",
    "\n",
    "\n",
    "# ### 3-2) 띄어쓰기 고치기\n",
    "#\n",
    "# ref : 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text) - 딥 러닝을 이용한 자연어 처리 입문 (wikidocs.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb56fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pykospacing import Spacing\n",
    "\n",
    "spacing = Spacing()\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(lambda x: spacing(x))\n",
    "\n",
    "\n",
    "# # 3. Embeding Vector 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "embedder = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "df['hf_embeddings'] = df['reviews'].apply(lambda x : embedder.encode(x))\n",
    "\n",
    "\n",
    "# # 4. 코사인 유사도(Cosine Similarity) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163f6b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "def get_query_sim_top_k(query, model, df, top_k):\n",
    "    # Encode the query\n",
    "    query_encode = model.encode(query)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos_scores = util.pytorch_cos_sim(query_encode, df['hf_embeddings'])[0]\n",
    "\n",
    "    # Get top k results\n",
    "    top_results_idx = torch.topk(cos_scores, k=top_k).indices.cpu().numpy()\n",
    "    top_results_df = df.iloc[top_results_idx][['product_name', 'product_link_url']]\n",
    "\n",
    "    return top_results_df\n",
    "\n",
    "get_query_sim_top_k(\"대학생들이 사용하기에 좋은 노트북\", model, df, 5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
